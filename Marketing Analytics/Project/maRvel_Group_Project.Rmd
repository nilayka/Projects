---
title: "maRvel Group Project"
author: 
date: "5/20/2020"
output: 
    html_document:
      code_folding: hide
      theme: united
      number_sections: true 
      toc: true # table of content true
      toc_depth: 2  # upto two depths of headings (specified by #, ## and ###) 
      highlight: tango  # specifies the syntax highlighting style
---

Group Members:

* [Nilay Kamar](https://www.linkedin.com/in/kamarnilay/)
* [Sezer Uluta≈ü](https://www.linkedin.com/in/sezerulutas/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("mlr")           #
library("tidyverse")     #
library("DataExplorer")  #
library("factoextra")    #
library("dendextend")    #
library("reshape2")      #
library("ggforce")       #
library("cluster")       #
library("dplyr")         #
library("corrplot")
library("cluster")
library("NbClust")
library("gridExtra")
library("GGally")
```

# Introduction, Definitions

This project is an example of using customer segmentation to define a marketing strategy. In this sample data set which summarizes the usage behavior of approximately 9000 active credit card holders in the last 6 months, our aim is to perform this segmentation in the most accurate way.

* CUSTID : Identification of Credit Card holder (Categorical)
* BALANCE : Balance amount left in their account to make purchases
* BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)
* PURCHASES : Amount of purchases made from account
* ONEOFFPURCHASES : Maximum purchase amount done in one-go
* INSTALLMENTSPURCHASES : Amount of purchase done in installment
* CASHADVANCE : Cash in advance given by the user
* PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)
* ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)
* PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)
* CASHADVANCEFREQUENCY : How frequently the cash in advance being paid
* CASHADVANCETRX : Number of Transactions made with "Cash in Advanced"
* PURCHASESTRX : Numbe of purchase transactions made
* CREDITLIMIT : Limit of Credit Card for user
* PAYMENTS : Amount of Payment done by user
* MINIMUM_PAYMENTS : Minimum amount of payments made by user
* PRCFULLPAYMENT : Percent of full payment paid by user
* TENURE : Tenure of credit card service for user

# Import Data

```{r pressure, echo=FALSE}
cc_data <- read.csv("/Users/nilaykamar/Downloads/CC GENERAL.csv", 
               stringsAsFactors = F,
               na.strings = c(" "))
head(cc_data)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

# EDA and Cleaning

```{r}
str(cc_data)
```

Hmm, all variables are numeric other than CUST_ID which will be removed for clustering.

```{r}
summary(cc_data)

sum(is.na(cc_data))   #Trying to find NA values.
```

Where is my missing NA values?

```{r}
plot_missing(cc_data)
```
I know where is my missing values, so I would like to replace that values with 0.
```{r}
cc_data$MINIMUM_PAYMENTS[which(is.na(cc_data$MINIMUM_PAYMENTS))]<- 0
summary(cc_data)
```

As we see we also see 1 NA value on CREDIT LIMIT. As we know from banking system, CREDIT LIMIT can not be 0, I would like to give a mean value to this NA.

```{r}
cc_data$CREDIT_LIMIT[which(is.na(cc_data$CREDIT_LIMIT))] <- mean(cc_data$CREDIT_LIMIT,
                                                                 na.rm=TRUE) 
summary(cc_data)

```
Re-checking my values again for NA detection.
```{r}
plot_missing(cc_data)
```

There is no missing values on my dataset. Yey!

```{r}
cc_data <- cc_data[, -1] #Removing my first column, will let me to play with this dataset correctly.
```

```{r}
str(cc_data) #That was what we want. 
```


```{r}
# Density Plot of frequency attributes

ggplot(cc_data ,aes(x=BALANCE_FREQUENCY))+
  geom_density(alpha=0.5, fill = "darkgreen") + 
  labs(title="Balance Frequency") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  theme_minimal()

ggplot(cc_data ,aes(x=ONEOFF_PURCHASES_FREQUENCY))+
  geom_density(alpha=0.5, fill = "darkgreen") + 
  labs(title="Oneoff Purchases Frequency") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  theme_minimal()

ggplot(cc_data ,aes(x=PURCHASES_FREQUENCY))+
  geom_density(alpha=0.5, fill = "darkgreen") + 
  labs(title="Purchases Frequency") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  theme_minimal()

ggplot(cc_data ,aes(x=PURCHASES_INSTALLMENTS_FREQUENCY))+
  geom_density(alpha=0.5, fill = "darkgreen") + 
  labs(title="Purchases Installments Frequency") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  theme_minimal()

ggplot(cc_data ,aes(x=CASH_ADVANCE_FREQUENCY))+
  geom_density(alpha=0.5, fill = "darkgreen") + 
  labs(title="Cash Advance Frequency") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  theme_minimal()
```

```{r}
# Historgram for each non-frequency attribute

freq <- c("BALANCE_FREQUENCY", "ONEOFF_PURCHASES_FREQUENCY", "PURCHASES_FREQUENCY",
         "PURCHASES_INSTALLMENTS_FREQUENCY", "CASH_ADVANCE_FREQUENCY")

cc_data %>%
  select(-freq) %>%
  gather(Attributes, value) %>% 
  ggplot(aes(x=value)) +
  geom_histogram(fill = "lightblue2", color = "black", stat = "count") + 
  facet_wrap(~Attributes, scales = "free_x") +
  labs(x = "Value", y = "Frequency",
      title="Credit Cards Attributes - Histograms") +
  theme(axis.text.x = element_text(colour = "grey0", size = 10, angle = 30, hjust = 0.5, vjust = 0.5),
        axis.text.y = element_text(colour = "grey20", size = 10),
        text = element_text(size = 12)) +
  theme_minimal()
```

```{r}
corrplot(cor(cc_data), diag = FALSE, type = "upper", order = "hclust",
         tl.col = "black", tl.pos = "td", tl.cex = 0.5, method = "circle")
```

```{r}
# Relationship between Balance and Cash Advance
ggplot(cc_data, aes(x=BALANCE, y=CASH_ADVANCE)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="Credit Cards Attributes",
       subtitle="Relationship between Balance and Cash Advance") +
  theme_bw()
```

```{r}
# Normalization
cc_scaled <- as.data.frame(scale(cc_data)) #It helps us to normalise the data within a particular range. (Also scaling gives us fast processing.)

# Original data
p1 <- ggplot(cc_data, aes(x=CREDIT_LIMIT, y=BALANCE)) +
  geom_point() +
  labs(title="Original data") +
  theme_bw()

# Normalized data 
p2 <- ggplot(cc_scaled, aes(x=CREDIT_LIMIT, y=BALANCE)) +
  geom_point() +
  labs(title="Normalized data") +
  theme_bw()

# Subplot
grid.arrange(p1, p2, ncol=2)
```
PCA Analysis
```{r}
res.pca <- prcomp(cc_scaled,  scale = TRUE)
# Visualize eigenvalues/variances
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

As shown graph above, 10 components cover nearly 80% of variance according to PCA analysis. 

```{r}
# Extract the results for variables
var <- get_pca_var(res.pca)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Control variable colors using their contributions to the principle axis
fviz_pca_var(res.pca, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping
             ) + theme_minimal() + ggtitle("Variables - PCA")
```

# KMeans Clustering

Kmeans is a simple unsupervised learning algorithm to cluster dataset into groups. Even if the cluster number is not optimal for the dataset, the algorithm splits into the given number of groups. It can be got different results even if the same code runs, because of changing the starting point of the model. Therefore randomization is the essential point of the Kmeans algorithm as the same as other algorithms.

We can start four centers for our model, and let's look at the results:

```{r}
set.seed(96743)        # because starting assignments are random
k <- kmeans(cc_scaled, centers=4, nstart = 25)
```

```{r}
k$center
```

```{r}
clusplot(cc_data, k$cluster, color=TRUE, shade=TRUE, 
         labels=4, lines=0, main="K-means cluster plot")
```

The figure shown above plotted by two components which cover 47.62%  of the point variability. 

```{r}
# Cluster size
k$size
```

There is a problem that must be solved because the distribution of the size between clusters seems unbalanced. There may be several reasons for this, we could start with control the number of clusters whether the cluster number is true. 

There are different methods to find the optimal cluster number in the Kmeans algorithm. We focus on them in the next section to find the best cluster number.

## Determining Optimal Clusters

There are several popular methods to find optimal clusters. We applied two methods listed below:

* Elbow Method
* Silhouette Method

They are explained with their examples respectively below.

### Elbow Method

In the Elbow method, it is plotted a line chart using within-groups the sum of squares of the model which is applied the Kmeans algorithm with the range of different k values. Because a line plot seems like an arm, this method is named Elbow Method. SSE is decreasing as increasing k value because the distance between clusters decreases. Our goal is to choose k value corresponding to the "Elbow point" in the chart. This point is also the minimum SSE value for our model. 

```{r}
wss <- (nrow(cc_data)-1)*sum(apply(cc_data,2,var))

for (i in 2:15) {
    wss[i] <- sum(kmeans(cc_data, centers=i)$tot.withinss)
    }

plot(1:15, wss, type="b", pch = 19, frame = FALSE,
     xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```

```{r}
fviz_cluster(k, data = cc_scaled)
```

```{r}
fviz_nbclust(cc_scaled, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
    labs(subtitle = "Elbow method")
```

According to the Elbow Method, the elbow point of the model seems seven cluster is the best choice. 

### Silhouette Method

In the Silhouette method, it is plotted a line chart like an Elbow Method using the range of different k values. While the Elbow Method tries to calculate the error between the clusters, the Silhouette Method tries to estimate the average distance between clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. therefore, our goal is to decide k value corresponding to a higher average distance between clusters.

```{r}
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
  km.res <- kmeans(cc_scaled, centers = k, nstart = 25, iter.max = 50)
  ss <- silhouette(km.res$cluster, dist(cc_scaled))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
```

With a different visualization technique for Silhouette Method:

```{r}
fviz_nbclust(cc_scaled, kmeans, method = "silhouette")
```

The seven clusters seems the optimal number for our model.

# Results

It seems seven clusters are good for our model. 

```{r}
# Execution of k-means with k=7
set.seed(1234)

seg.k7 <- kmeans(cc_scaled, centers=7)

# Mean values of each cluster
aggregate(cc_data, by=list(seg.k7$cluster), mean)
```

```{r}
seg.k7$size
```

```{r message=FALSE, warning=TRUE, paged.print=TRUE}
# Clustering 
ggpairs(cbind(cc_data, Cluster=as.factor(seg.k7$cluster)),
        columns=1:6, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        upper=list(continuous="blank"),
        axisLabels="none", switch="both") +
        theme_bw()
```

```{r}
seg.k7$size
```

```{r}
fviz_cluster(seg.k7, data = cc_scaled)
```

# Summary

The aim was to segment customers to define a marketing strategy. Below you can see the user groups that we have segmented according to the clusters we have reserved.

* Cluster 1 - Subclass user group: They are the users with the lowest cash. Credit limits are also low. They don't buy often. They do not perform banking transactions too much and their balances are not updated frequently.

* Cluster 2 - Balanced middle-class user group: Users who are in the 2nd largest group in this regard when they are considered as one-time purchases, with no significant differences between cash and spending expenses. Installment amounts are above average and it is the second largest group in this regard. The number of purchases is quite high. They are in Group 2 in this field as well. They have an average credit limit.

* Cluster 3 - User group with the smallest expenditures: They have the third highest balance. They work in advance. They have an average credit limit. Credit card service periods are the lowest.

* Cluster 4 - Small Spending and User Group with the Lowest Credit Limit: These users are included in the group with the lowest credit limit, but they do not buy too much. They are the largest customer group.

* Cluster 5 - Credit card lovers group: It is the lowest cash-buying group. They make an average of 80% of their repayments. This group is the third smallest in number and they do not keep their money in the bank.

* Cluster 6 - User group with big expenditures: It is the second user group with the highest cash. They make expensive purchases and have the highest credit limit. It is the smallest customer group. 7. Users in the group are candidates for entering this group. Loyalty bank applications can be implemented to increase spending.

* Cluster 7 - Frugal user group w/ money: It is the group with the highest cash. They generally like to trade with cash advance. They pay attention to their balances and expenses at the bank. It is the group that makes the highest payment in terms of reimbursement. Marketing transactions can be carried out in order to ensure that the transactions made in cash are returned to the credit card.

# References

[1] https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/

[2] https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92